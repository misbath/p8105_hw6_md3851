---
title: "p8105_hw6_md3851"
author: "Misbath Daouda"
date: "11/19/2019"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(broom)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

#Problem 1 

###Model Selection

```{r}
bwt_data = read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>%
  mutate( 
    babysex = as.factor(babysex),
    babysex = recode(babysex, "1" = "male", "2" = "female"),
    frace = as.factor(frace),
    frace = recode(frace, "1" = "white", "2" = "black", "3" = "asian", "4" = "puerto rican", "8" = "other", "9" = "unkown"),
    malform = as.factor(malform),
    malform = recode(malform, "0" = "absent", "1" = "present"),
    mrace = as.factor(mrace),
    mrace = recode(mrace, "1" = "white", "2" = "black", "3" = "asian", "4" = "puerto rican", "8" = "other")
    ) %>%
  drop_na() 

bwt_data
```

```{r}
model_selection = lm(bwt ~., data = bwt_data) %>% 
  MASS::stepAIC(direction = "both", trace = FALSE) %>% 
  summary()

model_selection
```

Model selection was automated with the stepAIC function from the MASS package. The direction was set to "both" so that both forward and backward selection were completed. The results return the best model, which is then labeled as model_1 below. 

###Residual Plots 

```{r}

model_1 = lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = bwt_data)

bwt_data %>% 
  add_residuals(model_1) %>%
  add_predictions(model_1) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  labs(
       title = "Model_1 Residual Plot", 
       x = "Predictions",
       y = "Standardized Residuals")

model_2 = lm(bwt ~ blength + gaweeks, data = bwt_data)

bwt_data %>% 
  add_residuals(model_2) %>%
  add_predictions(model_2) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  labs(
       title = "Model_2 Residual Plot", 
       x = "Predictions",
       y = "Standardized Residuals")

model_3 = lm(bwt ~ bhead + blength + babysex +  bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = bwt_data)

bwt_data %>% 
  add_residuals(model_3) %>%
  add_predictions(model_3) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() +
  labs(
       title = "Model_3 Residual Plot", 
       x = "Predictions",
       y = "Standardized Residuals")
```

The three models and their associated residual plots are presented above. 

###Cross validation

```{r}
cv_df = 
  crossv_mc(bwt_data, 100)

cv_df = cv_df %>% 
  mutate(model_1  = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
         model_2  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         model_3  = map(train, ~lm(bwt ~ bhead + blength + babysex +  bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(rmse_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
         rmse_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
         rmse_3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y)))

cv_plot = cv_df %>% 
  select(starts_with("rmse"))%>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()

cv_plot 
```

The cross-validation plot above shows the distribution of RMSE values for each candidate model. The RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data or how close the observed data points are to the modelâ€™s predicted values. Given that model_1 has the lowest RMSE values, this suggests that it performs better than the two other models. Similarly, model_2 performs better than model_3. 

#Problem 2

###Creating bootstraps samples 

```{r, message = FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

boot_straps_tidy = weather_df %>% 
  bootstrap(n=5000) %>% 
  mutate(models = map(strap, ~lm(tmax ~ tmin, data = .x)),
         results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(-std.error, -statistic, -p.value) %>%
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  rename(beta1 = tmin, beta0 = `(Intercept)`) %>% 
  mutate(log = log(beta0*beta1))

boot_straps_tidy

boot_straps_glance = weather_df %>% 
  bootstrap(n=5000) %>% 
  mutate(models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
         results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(.id, r.squared)

boot_straps_glance
```

###Visualizing distribution of estimates 

```{r plots}
log_plot = boot_straps_tidy %>% 
  ggplot(aes(x = log)) +
    geom_density() + 
    labs(
    x = "Log (Beta0*Beta1) Estimate",
    y = "Density",
    title = "Distribution of Log (Beta0*Beta1) Estimates")

log_plot
```

The estimates for the log value follow an approximately normal distribution, except for a small divot at the very top of the peak. There is also a larger tail towards the lower values than higher values which could indicate that large outliers are excluded from the bootstrap more than smaller outliers. 

```{r}
r_sq_plot = boot_straps_glance %>% 
  ggplot(aes(x = r.squared)) +
    geom_density() + 
    labs(
    x = "R squared Estimate",
    y = "Density",
    title = "Distribution of R Squared Estimates"
    )
r_sq_plot 

```

The estimates for the r squared value also follow a distribution close to normal, with a larger tail towards the lower values than higher values. This could indicate that large outliers are excluded from the bootstrap more than smaller outliers. 


###Determining 95% CI

```{r CIs}
CI_rsquared = 
  boot_straps_glance %>% 
    summarize(
    perc_2.5 = round(quantile(r.squared, 0.025),2),
    perc_97.5 = round(quantile(r.squared, 0.975),2)) %>%
  knitr::kable()

CI_rsquared

CI_log = 
  boot_straps_tidy %>% 
    summarize(
    perc_2.5 = round(quantile(log, 0.025),2),
    perc_97.5 = round(quantile(log, 0.975),2)) %>%
  knitr::kable()

CI_log


```

The 95% CI for the log estimates ranges from 1.96-2.05. 

The 95% CI for the rsquared estimates ranges from 0.89-0.93. 
  



